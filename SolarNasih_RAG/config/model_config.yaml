# Text Models
text_models:
  default: "all-MiniLM-L6-v2"
  alternatives:
    - "all-mpnet-base-v2"
    - "sentence-transformers/all-MiniLM-L12-v2"
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Image Models
image_models:
  default: "clip-ViT-B-32"
  alternatives:
    - "clip-ViT-L-14"
    - "blip-image-captioning-base"

# Audio Models
audio_models:
  default: "whisper-base"
  alternatives:
    - "whisper-small"
    - "whisper-medium"

# LLM Models
llm_models:
  openai:
    default: "gpt-3.5-turbo"
    alternatives:
      - "gpt-4"
      - "gpt-4-turbo-preview"
  anthropic:
    default: "claude-3-sonnet-20240229"
    alternatives:
      - "claude-3-opus-20240229"
      - "claude-3-haiku-20240307"
  huggingface:
    default: "microsoft/DialoGPT-medium"
    alternatives:
      - "microsoft/DialoGPT-large"
      - "facebook/blenderbot-400M-distill"

# Chunking Configuration
chunking:
  text:
    chunk_size: 512
    chunk_overlap: 50
    separators: ["\n\n", "\n", " ", ""]
  image:
    max_patches: 16
    patch_size: 224
  audio:
    segment_duration: 30
    overlap_duration: 5

# Embedding Configuration
embeddings:
  text:
    model_name: "all-MiniLM-L6-v2"
    dimensions: 384
    normalize: true
  image:
    model_name: "clip-ViT-B-32"
    dimensions: 512
    normalize: true
  audio:
    model_name: "whisper-base"
    dimensions: 512
    normalize: true