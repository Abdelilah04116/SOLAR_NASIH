"""
Agent RAG multimodal avec LangGraph et Gemini 2.0
Architecture basÃ©e sur le diagramme fourni avec MEM (Memory), Query Expansion, Re-ranking
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from dataclasses import dataclass
from datetime import datetime
import json

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Gemini imports
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# Imports locaux
from models import SearchResult, SearchQuery
from pipeline import MultimodalRAGPipeline
from config import RAGPipelineConfig

logger = logging.getLogger(__name__)

# ==================== Configuration Gemini ====================

@dataclass
class GeminiConfig:
    """Configuration pour Gemini 2.0"""
    api_key: str
    model_name: str = "gemini-2.0-flash-exp"
    temperature: float = 0.7
    max_output_tokens: int = 2048
    safety_settings: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.safety_settings is None:
            self.safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            }

class GeminiLLM:
    """Interface Gemini 2.0 pour la gÃ©nÃ©ration de rÃ©ponses"""
    
    def __init__(self, config: GeminiConfig):
        self.config = config
        genai.configure(api_key=config.api_key)
        
        self.model = genai.GenerativeModel(
            model_name=config.model_name,
            safety_settings=config.safety_settings
        )
        
        # Configuration de gÃ©nÃ©ration
        self.generation_config = genai.types.GenerationConfig(
            temperature=config.temperature,
            max_output_tokens=config.max_output_tokens,
            top_p=0.8,
            top_k=20
        )
    
    async def generate_response(self, prompt: str, context: str = "") -> str:
        """GÃ©nÃ¨re une rÃ©ponse avec Gemini"""
        try:
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            response = await asyncio.to_thread(
                self.model.generate_content,
                full_prompt,
                generation_config=self.generation_config
            )
            
            return response.text
        except Exception as e:
            logger.error(f"Erreur Gemini: {e}")
            return f"Erreur lors de la gÃ©nÃ©ration: {str(e)}"
    
    def generate_response_sync(self, prompt: str, context: str = "") -> str:
        """Version synchrone de la gÃ©nÃ©ration"""
        try:
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=self.generation_config
            )
            
            return response.text
        except Exception as e:
            logger.error(f"Erreur Gemini: {e}")
            return f"Erreur lors de la gÃ©nÃ©ration: {str(e)}"

# ==================== Ã‰tats LangGraph ====================

class AgentState(TypedDict):
    """Ã‰tat de l'agent RAG"""
    # EntrÃ©es utilisateur
    user_query: str
    conversation_history: List[BaseMessage]
    
    # Expansion de requÃªte
    expanded_queries: List[str]
    search_intent: str
    
    # RÃ©cupÃ©ration
    search_results: List[SearchResult]
    relevant_chunks: List[Dict[str, Any]]
    
    # MÃ©moire et contexte
    memory_context: Dict[str, Any]
    session_id: str
    
    # GÃ©nÃ©ration
    generated_response: str
    confidence_score: float
    
    # MÃ©tadonnÃ©es
    processing_steps: List[str]
    timestamp: str
    iteration_count: int

# ==================== Composants SpÃ©cialisÃ©s ====================

class QueryExpander:
    """Composant d'expansion de requÃªte utilisant Gemini"""
    
    def __init__(self, gemini_llm: GeminiLLM):
        self.llm = gemini_llm
        
        self.expansion_prompt = """
        Vous Ãªtes un expert en expansion de requÃªtes pour la recherche d'informations.
        
        RequÃªte originale: {query}
        
        TÃ¢ches:
        1. Identifiez l'intention de recherche (information, comparaison, procÃ©dure, etc.)
        2. GÃ©nÃ©rez 3-5 requÃªtes alternatives qui capturent diffÃ©rents aspects de la question
        3. Incluez des synonymes et des reformulations
        4. Considerez les termes techniques et vulgarisÃ©s
        
        Format de rÃ©ponse:
        INTENTION: [intention de recherche]
        REQUETES:
        1. [requÃªte alternative 1]
        2. [requÃªte alternative 2]
        3. [requÃªte alternative 3]
        4. [requÃªte alternative 4]
        5. [requÃªte alternative 5]
        """
    
    def expand_query(self, query: str) -> Dict[str, Any]:
        """Expanse une requÃªte en plusieurs variantes"""
        try:
            prompt = self.expansion_prompt.format(query=query)
            response = self.llm.generate_response_sync(prompt)
            
            # Parse de la rÃ©ponse
            lines = response.strip().split('\n')
            intention = ""
            expanded_queries = []
            
            for line in lines:
                if line.startswith("INTENTION:"):
                    intention = line.replace("INTENTION:", "").strip()
                elif line.strip() and (line[0].isdigit() or line.startswith("-")):
                    # Nettoyer la requÃªte (enlever numÃ©ros et tirets)
                    clean_query = line.split(".", 1)[-1].strip()
                    clean_query = clean_query.lstrip("- ").strip()
                    if clean_query:
                        expanded_queries.append(clean_query)
            
            return {
                "intention": intention,
                "expanded_queries": expanded_queries[:5],  # Limiter Ã  5
                "original_query": query
            }
            
        except Exception as e:
            logger.error(f"Erreur expansion requÃªte: {e}")
            return {
                "intention": "information",
                "expanded_queries": [query],
                "original_query": query
            }

class ResponseGenerator:
    """GÃ©nÃ©rateur de rÃ©ponse final utilisant Gemini"""
    
    def __init__(self, gemini_llm: GeminiLLM):
        self.llm = gemini_llm
        
        self.response_prompt = """
        Vous Ãªtes un assistant IA expert qui aide les utilisateurs en rÃ©pondant Ã  leurs questions 
        basÃ©es sur des documents fournis.
        
        CONTEXTE RÃ‰CUPÃ‰RÃ‰:
        {context}
        
        QUESTION UTILISATEUR: {query}
        
        INSTRUCTIONS:
        1. RÃ©pondez de maniÃ¨re prÃ©cise et complÃ¨te Ã  la question
        2. Basez votre rÃ©ponse sur les informations fournies dans le contexte
        3. Si les informations sont insuffisantes, indiquez-le clairement
        4. Structurez votre rÃ©ponse de maniÃ¨re claire avec des sections si nÃ©cessaire
        5. Citez les sources pertinentes (pages, sections) quand c'est possible
        6. Soyez objectif et factuel
        
        RÃ‰PONSE:
        """
        
        self.synthesis_prompt = """
        Vous devez synthÃ©tiser plusieurs Ã©lÃ©ments d'information pour rÃ©pondre Ã  une question.
        
        QUESTION: {query}
        INTENTION: {intention}
        
        INFORMATIONS DISPONIBLES:
        {chunks_info}
        
        CrÃ©ez une rÃ©ponse cohÃ©rente qui:
        1. RÃ©pond directement Ã  la question
        2. IntÃ¨gre toutes les informations pertinentes
        3. RÃ©sout les Ã©ventuelles contradictions
        4. Fournit une perspective complÃ¨te
        5. Structure l'information de maniÃ¨re logique
        
        RÃ‰PONSE SYNTHÃ‰TISÃ‰E:
        """
    
    def generate_response(self, query: str, chunks: List[Dict[str, Any]], 
                         intention: str = "") -> Dict[str, Any]:
        """GÃ©nÃ¨re une rÃ©ponse basÃ©e sur les chunks rÃ©cupÃ©rÃ©s"""
        try:
            # PrÃ©parer le contexte
            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                chunk_info = f"""
                [Source {i} - Page {chunk.get('page_number', 'N/A')}]
                Type: {chunk.get('chunk_type', 'text')}
                Contenu: {chunk.get('content', '')[:500]}...
                Score: {chunk.get('score', 0):.3f}
                """
                context_parts.append(chunk_info)
            
            context = "\n".join(context_parts)
            
            # Choisir le prompt selon le nombre de chunks
            if len(chunks) > 3:
                # Utiliser le prompt de synthÃ¨se pour plusieurs sources
                chunks_info = "\n".join([
                    f"â€¢ {chunk.get('content', '')[:200]}... (Page {chunk.get('page_number', 'N/A')})"
                    for chunk in chunks
                ])
                
                prompt = self.synthesis_prompt.format(
                    query=query,
                    intention=intention,
                    chunks_info=chunks_info
                )
            else:
                # Utiliser le prompt simple
                prompt = self.response_prompt.format(
                    context=context,
                    query=query
                )
            
            # GÃ©nÃ©rer la rÃ©ponse
            response = self.llm.generate_response_sync(prompt)
            
            # Calculer un score de confiance basique
            confidence = self._calculate_confidence(chunks, response)
            
            return {
                "response": response,
                "confidence": confidence,
                "sources_used": len(chunks),
                "context_length": len(context)
            }
            
        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration rÃ©ponse: {e}")
            return {
                "response": f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}",
                "confidence": 0.0,
                "sources_used": 0,
                "context_length": 0
            }
    
    def _calculate_confidence(self, chunks: List[Dict[str, Any]], response: str) -> float:
        """Calcule un score de confiance basique"""
        if not chunks:
            return 0.0
        
        # Facteurs de confiance
        avg_score = sum(chunk.get('score', 0) for chunk in chunks) / len(chunks)
        num_sources = min(len(chunks) / 5, 1.0)  # Bonus pour plus de sources
        response_length = min(len(response) / 500, 1.0)  # Bonus pour rÃ©ponses dÃ©taillÃ©es
        
        # Score composite
        confidence = (avg_score * 0.5 + num_sources * 0.3 + response_length * 0.2)
        return min(confidence, 1.0)

class MemoryManager:
    """Gestionnaire de mÃ©moire conversationnelle"""
    
    def __init__(self):
        self.sessions = {}
        self.max_history_length = 10
    
    def get_session_context(self, session_id: str) -> Dict[str, Any]:
        """RÃ©cupÃ¨re le contexte d'une session"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                "conversation_history": [],
                "topics": [],
                "preferences": {},
                "created_at": datetime.now().isoformat()
            }
        
        return self.sessions[session_id]
    
    def update_session(self, session_id: str, query: str, response: str, 
                      topics: List[str] = None):
        """Met Ã  jour le contexte de session"""
        context = self.get_session_context(session_id)
        
        # Ajouter Ã  l'historique
        context["conversation_history"].append({
            "query": query,
            "response": response,
            "timestamp": datetime.now().isoformat()
        })
        
        # Limiter la taille de l'historique
        if len(context["conversation_history"]) > self.max_history_length:
            context["conversation_history"] = context["conversation_history"][-self.max_history_length:]
        
        # Mettre Ã  jour les sujets
        if topics:
            for topic in topics:
                if topic not in context["topics"]:
                    context["topics"].append(topic)
        
        # Limiter les sujets
        context["topics"] = context["topics"][-20:]
    
    def get_context_summary(self, session_id: str) -> str:
        """GÃ©nÃ¨re un rÃ©sumÃ© du contexte pour l'IA"""
        context = self.get_session_context(session_id)
        
        if not context["conversation_history"]:
            return ""
        
        # RÃ©cupÃ©rer les derniÃ¨res interactions
        recent_history = context["conversation_history"][-3:]
        
        summary_parts = []
        if context["topics"]:
            summary_parts.append(f"Sujets abordÃ©s: {', '.join(context['topics'][-5:])}")
        
        if recent_history:
            summary_parts.append("DerniÃ¨res interactions:")
            for item in recent_history:
                summary_parts.append(f"Q: {item['query'][:100]}...")
                summary_parts.append(f"R: {item['response'][:100]}...")
        
        return "\n".join(summary_parts)

# ==================== NÅ“uds LangGraph ====================

class RAGAgent:
    """Agent RAG principal utilisant LangGraph"""
    
    def __init__(self, rag_pipeline: MultimodalRAGPipeline, gemini_config: GeminiConfig):
        self.rag_pipeline = rag_pipeline
        self.gemini_llm = GeminiLLM(gemini_config)
        
        # Composants spÃ©cialisÃ©s
        self.query_expander = QueryExpander(self.gemini_llm)
        self.response_generator = ResponseGenerator(self.gemini_llm)
        self.memory_manager = MemoryManager()
        
        # Configuration du graphe
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Construit le graphe LangGraph de l'agent"""
        workflow = StateGraph(AgentState)
        
        # Ajout des nÅ“uds
        workflow.add_node("initialize", self._initialize_node)
        workflow.add_node("expand_query", self._expand_query_node)
        workflow.add_node("retrieve", self._retrieve_node)
        workflow.add_node("rerank", self._rerank_node)
        workflow.add_node("generate", self._generate_node)
        workflow.add_node("finalize", self._finalize_node)
        
        # DÃ©finition du flux
        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "expand_query")
        workflow.add_edge("expand_query", "retrieve")
        workflow.add_edge("retrieve", "rerank")
        workflow.add_edge("rerank", "generate")
        workflow.add_edge("generate", "finalize")
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
    
    def _initialize_node(self, state: AgentState) -> AgentState:
        """NÅ“ud d'initialisation"""
        state["processing_steps"] = ["initialization"]
        state["timestamp"] = datetime.now().isoformat()
        state["iteration_count"] = 0
        
        # RÃ©cupÃ©rer le contexte de mÃ©moire
        session_id = state.get("session_id", "default")
        memory_context = self.memory_manager.get_session_context(session_id)
        state["memory_context"] = memory_context
        
        logger.info(f"Agent initialisÃ© pour requÃªte: {state['user_query'][:50]}...")
        return state
    
    def _expand_query_node(self, state: AgentState) -> AgentState:
        """NÅ“ud d'expansion de requÃªte"""
        state["processing_steps"].append("query_expansion")
        
        try:
            expansion_result = self.query_expander.expand_query(state["user_query"])
            
            state["expanded_queries"] = expansion_result["expanded_queries"]
            state["search_intent"] = expansion_result["intention"]
            
            logger.info(f"RequÃªte expansÃ©e: {len(state['expanded_queries'])} variantes")
            
        except Exception as e:
            logger.error(f"Erreur expansion: {e}")
            state["expanded_queries"] = [state["user_query"]]
            state["search_intent"] = "information"
        
        return state
    
    def _retrieve_node(self, state: AgentState) -> AgentState:
        """NÅ“ud de rÃ©cupÃ©ration de documents"""
        state["processing_steps"].append("retrieval")
        
        all_results = []
        
        # Rechercher avec la requÃªte originale et les expansions
        queries_to_search = [state["user_query"]] + state.get("expanded_queries", [])
        
        for query in queries_to_search[:4]:  # Limiter Ã  4 requÃªtes max
            try:
                results = self.rag_pipeline.search(query, k=3)
                all_results.extend(results)
            except Exception as e:
                logger.error(f"Erreur recherche '{query}': {e}")
        
        # DÃ©duplication basÃ©e sur chunk_id
        unique_results = {}
        for result in all_results:
            if result.chunk_id not in unique_results:
                unique_results[result.chunk_id] = result
            else:
                # Garder le meilleur score
                if result.score > unique_results[result.chunk_id].score:
                    unique_results[result.chunk_id] = result
        
        state["search_results"] = list(unique_results.values())
        
        logger.info(f"RÃ©cupÃ©ration: {len(state['search_results'])} chunks uniques trouvÃ©s")
        return state
    
    def _rerank_node(self, state: AgentState) -> AgentState:
        """NÅ“ud de re-ranking des rÃ©sultats"""
        state["processing_steps"].append("reranking")
        
        # Re-ranking simple basÃ© sur la pertinence et la diversitÃ©
        results = state["search_results"]
        
        if not results:
            state["relevant_chunks"] = []
            return state
        
        # Trier par score dÃ©croissant
        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)
        
        # SÃ©lectionner les meilleurs en favorisant la diversitÃ©
        selected_chunks = []
        seen_pages = set()
        seen_types = set()
        
        for result in sorted_results:
            chunk_dict = {
                "chunk_id": result.chunk_id,
                "content": result.content,
                "chunk_type": result.chunk_type.value,
                "page_number": result.page_number,
                "score": result.score,
                "metadata": result.metadata
            }
            
            # CritÃ¨res de sÃ©lection
            add_chunk = True
            
            # Favoriser la diversitÃ© des pages
            if len(selected_chunks) >= 3 and result.page_number in seen_pages:
                if result.score < 0.8:  # Seuil de score Ã©levÃ©
                    add_chunk = False
            
            # Favoriser la diversitÃ© des types
            if len(selected_chunks) >= 5 and result.chunk_type.value in seen_types:
                if result.score < 0.9:
                    add_chunk = False
            
            if add_chunk:
                selected_chunks.append(chunk_dict)
                seen_pages.add(result.page_number)
                seen_types.add(result.chunk_type.value)
            
            # Limiter Ã  8 chunks maximum
            if len(selected_chunks) >= 8:
                break
        
        state["relevant_chunks"] = selected_chunks
        
        logger.info(f"Re-ranking: {len(selected_chunks)} chunks sÃ©lectionnÃ©s")
        return state
    
    def _generate_node(self, state: AgentState) -> AgentState:
        """NÅ“ud de gÃ©nÃ©ration de rÃ©ponse"""
        state["processing_steps"].append("generation")
        
        try:
            generation_result = self.response_generator.generate_response(
                query=state["user_query"],
                chunks=state["relevant_chunks"],
                intention=state.get("search_intent", "")
            )
            
            state["generated_response"] = generation_result["response"]
            state["confidence_score"] = generation_result["confidence"]
            
            logger.info(f"RÃ©ponse gÃ©nÃ©rÃ©e (confiance: {generation_result['confidence']:.3f})")
            
        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration: {e}")
            state["generated_response"] = "DÃ©solÃ©, je n'ai pas pu gÃ©nÃ©rer une rÃ©ponse appropriÃ©e."
            state["confidence_score"] = 0.0
        
        return state
    
    def _finalize_node(self, state: AgentState) -> AgentState:
        """NÅ“ud de finalisation"""
        state["processing_steps"].append("finalization")
        state["iteration_count"] += 1
        
        # Mettre Ã  jour la mÃ©moire
        session_id = state.get("session_id", "default")
        self.memory_manager.update_session(
            session_id=session_id,
            query=state["user_query"],
            response=state["generated_response"]
        )
        
        logger.info("Traitement terminÃ©")
        return state
    
    async def process_query(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """Traite une requÃªte utilisateur"""
        # Ã‰tat initial
        initial_state = AgentState(
            user_query=query,
            session_id=session_id,
            conversation_history=[],
            expanded_queries=[],
            search_intent="",
            search_results=[],
            relevant_chunks=[],
            memory_context={},
            generated_response="",
            confidence_score=0.0,
            processing_steps=[],
            timestamp="",
            iteration_count=0
        )
        
        # ExÃ©cution du graphe
        try:
            final_state = await asyncio.to_thread(self.graph.invoke, initial_state)
            
            return {
                "response": final_state["generated_response"],
                "confidence": final_state["confidence_score"],
                "sources": final_state["relevant_chunks"],
                "processing_steps": final_state["processing_steps"],
                "expanded_queries": final_state.get("expanded_queries", []),
                "search_intent": final_state.get("search_intent", ""),
                "timestamp": final_state["timestamp"]
            }
        
        except Exception as e:
            logger.error(f"Erreur traitement requÃªte: {e}")
            return {
                "response": f"Erreur lors du traitement: {str(e)}",
                "confidence": 0.0,
                "sources": [],
                "processing_steps": ["error"],
                "expanded_queries": [],
                "search_intent": "",
                "timestamp": datetime.now().isoformat()
            }
    
    def process_query_sync(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """Version synchrone du traitement"""
        return asyncio.run(self.process_query(query, session_id))

# ==================== Factory et Interface ====================

def create_rag_agent(rag_pipeline: MultimodalRAGPipeline, 
                    gemini_api_key: str,
                    model_name: str = "gemini-2.0-flash-exp") -> RAGAgent:
    """Factory pour crÃ©er un agent RAG"""
    gemini_config = GeminiConfig(
        api_key=gemini_api_key,
        model_name=model_name
    )
    
    return RAGAgent(rag_pipeline, gemini_config)

# ==================== Exemple d'utilisation ====================

async def example_usage():
    """Exemple d'utilisation de l'agent RAG"""
    # Configuration
    rag_config = RAGPipelineConfig()
    rag_pipeline = MultimodalRAGPipeline(rag_config)
    
    # ClÃ© API Gemini (Ã  configurer)
    GEMINI_API_KEY = "your-gemini-api-key-here"
    
    # CrÃ©ation de l'agent
    agent = create_rag_agent(rag_pipeline, GEMINI_API_KEY)
    
    # Exemples de requÃªtes
    queries = [
        "Qu'est-ce que l'intelligence artificielle?",
        "Comment fonctionne un rÃ©seau de neurones?",
        "Quelles sont les applications du machine learning?",
        "Explique-moi le deep learning",
        "Quelle est la diffÃ©rence entre IA et ML?"
    ]
    
    session_id = "demo_session"
    
    for query in queries:
        print(f"\nğŸ¤– RequÃªte: {query}")
        print("=" * 50)
        
        result = await agent.process_query(query, session_id)
        
        print(f"ğŸ’­ Intention: {result['search_intent']}")
        print(f"ğŸ” RequÃªtes expansÃ©es: {result['expanded_queries']}")
        print(f"ğŸ“Š Confiance: {result['confidence']:.3f}")
        print(f"ğŸ“š Sources: {len(result['sources'])}")
        print(f"ğŸ“ RÃ©ponse: {result['response'][:200]}...")
        
        # Pause pour la dÃ©mo
        await asyncio.sleep(1)

if __name__ == "__main__":
    # Configuration du logging
    logging.basicConfig(level=logging.INFO)
    
    # Exemple d'exÃ©cution
    asyncio.run(example_usage())