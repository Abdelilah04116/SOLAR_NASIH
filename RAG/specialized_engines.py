"""
Moteurs spÃ©cialisÃ©s et extensions SMA pour l'agent RAG
Architecture modulaire pour ajouter des capacitÃ©s spÃ©cialisÃ©es
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional, Protocol, runtime_checkable
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
import json

# Imports pour les agents spÃ©cialisÃ©s
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

logger = logging.getLogger(__name__)

# ==================== Interfaces et Protocoles ====================

@runtime_checkable
class SpecializedEngine(Protocol):
    """Interface pour les moteurs spÃ©cialisÃ©s"""
    
    def can_handle(self, query: str, context: Dict[str, Any]) -> bool:
        """DÃ©termine si ce moteur peut traiter la requÃªte"""
        ...
    
    async def process(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Traite la requÃªte avec ce moteur spÃ©cialisÃ©"""
        ...
    
    def get_capabilities(self) -> List[str]:
        """Retourne les capacitÃ©s de ce moteur"""
        ...

class EngineType(Enum):
    """Types de moteurs spÃ©cialisÃ©s"""
    MATHEMATICAL = "mathematical"
    TECHNICAL = "technical"
    ANALYTICAL = "analytical"
    CREATIVE = "creative"
    FACTUAL = "factual"
    CONVERSATIONAL = "conversational"

@dataclass
class EngineResponse:
    """RÃ©ponse d'un moteur spÃ©cialisÃ©"""
    content: str
    confidence: float
    engine_type: EngineType
    processing_time: float
    metadata: Dict[str, Any]
    sources_used: List[str] = None

# ==================== Moteurs SpÃ©cialisÃ©s ====================

class MathematicalEngine:
    """Moteur spÃ©cialisÃ© pour les questions mathÃ©matiques et calculs"""
    
    def __init__(self, gemini_llm):
        self.llm = gemini_llm
        self.engine_type = EngineType.MATHEMATICAL
        
        self.math_prompt = """
        Vous Ãªtes un expert en mathÃ©matiques et calculs. RÃ©pondez aux questions mathÃ©matiques
        avec prÃ©cision et clartÃ©.
        
        Question: {query}
        Contexte disponible: {context}
        
        Instructions:
        1. Fournissez des calculs Ã©tape par Ã©tape
        2. Expliquez le raisonnement mathÃ©matique
        3. Utilisez des notations mathÃ©matiques claires
        4. VÃ©rifiez vos calculs
        5. Indiquez les limites ou approximations
        
        RÃ©ponse mathÃ©matique:
        """
        
        self.math_keywords = [
            'calcul', 'Ã©quation', 'formule', 'mathÃ©matique', 'statistique',
            'probabilitÃ©', 'algÃ¨bre', 'gÃ©omÃ©trie', 'trigonomÃ©trie',
            'dÃ©rivÃ©e', 'intÃ©grale', 'fonction', 'graphique', 'analyse',
            'nombre', 'pourcentage', 'ratio', 'moyenne', 'mÃ©diane'
        ]
    
    def can_handle(self, query: str, context: Dict[str, Any]) -> bool:
        """DÃ©termine si la requÃªte est mathÃ©matique"""
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in self.math_keywords)
    
    async def process(self, query: str, context: Dict[str, Any]) -> EngineResponse:
        """Traite une requÃªte mathÃ©matique"""
        import time
        start_time = time.time()
        
        try:
            # PrÃ©parer le contexte mathÃ©matique
            math_context = self._extract_math_context(context)
            
            prompt = self.math_prompt.format(
                query=query,
                context=math_context
            )
            
            response = await self.llm.generate_response(prompt)
            
            processing_time = time.time() - start_time
            
            return EngineResponse(
                content=response,
                confidence=0.9,  # Haute confiance pour les calculs
                engine_type=self.engine_type,
                processing_time=processing_time,
                metadata={
                    "math_keywords_found": [kw for kw in self.math_keywords if kw in query.lower()],
                    "context_type": "mathematical"
                }
            )
            
        except Exception as e:
            logger.error(f"Erreur moteur mathÃ©matique: {e}")
            return EngineResponse(
                content=f"Erreur lors du traitement mathÃ©matique: {str(e)}",
                confidence=0.0,
                engine_type=self.engine_type,
                processing_time=time.time() - start_time,
                metadata={"error": str(e)}
            )
    
    def _extract_math_context(self, context: Dict[str, Any]) -> str:
        """Extrait le contexte pertinent pour les mathÃ©matiques"""
        math_context_parts = []
        
        chunks = context.get('relevant_chunks', [])
        for chunk in chunks:
            content = chunk.get('content', '')
            # Rechercher des formules, nombres, tableaux
            if any(indicator in content.lower() for indicator in ['=', '%', 'tableau', 'graphique', 'donnÃ©es']):
                math_context_parts.append(f"Source: {content[:300]}...")
        
        return "\n".join(math_context_parts) if math_context_parts else "Aucun contexte mathÃ©matique spÃ©cifique."
    
    def get_capabilities(self) -> List[str]:
        """Retourne les capacitÃ©s mathÃ©matiques"""
        return [
            "Calculs arithmÃ©tiques",
            "RÃ©solution d'Ã©quations",
            "Analyse statistique",
            "GÃ©omÃ©trie et trigonomÃ©trie",
            "Analyse de donnÃ©es numÃ©riques"
        ]

class TechnicalEngine:
    """Moteur spÃ©cialisÃ© pour les questions techniques et d'ingÃ©nierie"""
    
    def __init__(self, gemini_llm):
        self.llm = gemini_llm
        self.engine_type = EngineType.TECHNICAL
        
        self.technical_prompt = """
        Vous Ãªtes un expert technique avec une expertise approfondie en ingÃ©nierie,
        informatique, et sciences appliquÃ©es.
        
        Question technique: {query}
        Documentation disponible: {context}
        
        Instructions:
        1. Fournissez des explications techniques prÃ©cises
        2. Incluez des dÃ©tails d'implÃ©mentation si pertinent
        3. Mentionnez les bonnes pratiques
        4. Identifiez les risques ou limitations
        5. SuggÃ©rez des ressources additionnelles
        
        RÃ©ponse technique dÃ©taillÃ©e:
        """
        
        self.technical_keywords = [
            'algorithme', 'architecture', 'systÃ¨me', 'rÃ©seau', 'base de donnÃ©es',
            'programmation', 'code', 'framework', 'api', 'protocole',
            'sÃ©curitÃ©', 'performance', 'optimisation', 'debug', 'test',
            'infrastructure', 'cloud', 'serveur', 'client', 'interface'
        ]
    
    def can_handle(self, query: str, context: Dict[str, Any]) -> bool:
        """DÃ©termine si la requÃªte est technique"""
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in self.technical_keywords)
    
    async def process(self, query: str, context: Dict[str, Any]) -> EngineResponse:
        """Traite une requÃªte technique"""
        import time
        start_time = time.time()
        
        try:
            # Analyser le type de question technique
            tech_type = self._analyze_technical_type(query)
            
            # PrÃ©parer le contexte technique
            tech_context = self._extract_technical_context(context, tech_type)
            
            prompt = self.technical_prompt.format(
                query=query,
                context=tech_context
            )
            
            response = await self.llm.generate_response(prompt)
            
            processing_time = time.time() - start_time
            
            return EngineResponse(
                content=response,
                confidence=0.85,
                engine_type=self.engine_type,
                processing_time=processing_time,
                metadata={
                    "technical_type": tech_type,
                    "keywords_found": [kw for kw in self.technical_keywords if kw in query.lower()]
                }
            )
            
        except Exception as e:
            logger.error(f"Erreur moteur technique: {e}")
            return EngineResponse(
                content=f"Erreur lors du traitement technique: {str(e)}",
                confidence=0.0,
                engine_type=self.engine_type,
                processing_time=time.time() - start_time,
                metadata={"error": str(e)}
            )
    
    def _analyze_technical_type(self, query: str) -> str:
        """Analyse le type de question technique"""
        query_lower = query.lower()
        
        if any(kw in query_lower for kw in ['code', 'programmation', 'algorithme']):
            return "programming"
        elif any(kw in query_lower for kw in ['rÃ©seau', 'protocole', 'tcp', 'http']):
            return "networking"
        elif any(kw in query_lower for kw in ['base de donnÃ©es', 'sql', 'requÃªte']):
            return "database"
        elif any(kw in query_lower for kw in ['sÃ©curitÃ©', 'cryptage', 'authentification']):
            return "security"
        else:
            return "general_technical"
    
    def _extract_technical_context(self, context: Dict[str, Any], tech_type: str) -> str:
        """Extrait le contexte technique pertinent"""
        chunks = context.get('relevant_chunks', [])
        tech_chunks = []
        
        for chunk in chunks:
            content = chunk.get('content', '').lower()
            # Filtrer selon le type technique
            if tech_type == "programming" and any(kw in content for kw in ['code', 'fonction', 'class']):
                tech_chunks.append(chunk.get('content', ''))
            elif tech_type == "networking" and any(kw in content for kw in ['rÃ©seau', 'protocole', 'tcp']):
                tech_chunks.append(chunk.get('content', ''))
            else:
                tech_chunks.append(chunk.get('content', ''))
        
        return "\n".join(tech_chunks[:3])  # Limiter Ã  3 chunks les plus pertinents
    
    def get_capabilities(self) -> List[str]:
        return [
            "Architecture logicielle",
            "Programmation et algorithmes",
            "RÃ©seaux et protocoles",
            "Bases de donnÃ©es",
            "SÃ©curitÃ© informatique",
            "Optimisation de performance"
        ]

class AnalyticalEngine:
    """Moteur spÃ©cialisÃ© pour l'analyse de donnÃ©es et insights"""
    
    def __init__(self, gemini_llm):
        self.llm = gemini_llm
        self.engine_type = EngineType.ANALYTICAL
        
        self.analytical_prompt = """
        Vous Ãªtes un expert en analyse de donnÃ©es avec une expertise en statistiques,
        business intelligence et data science.
        
        Demande d'analyse: {query}
        DonnÃ©es disponibles: {context}
        
        Instructions:
        1. Analysez les donnÃ©es de maniÃ¨re structurÃ©e
        2. Identifiez les tendances et patterns
        3. Fournissez des insights actionnables
        4. Quantifiez vos observations quand possible
        5. SuggÃ©rez des analyses complÃ©mentaires
        
        Analyse dÃ©taillÃ©e:
        """
        
        self.analytical_keywords = [
            'analyse', 'tendance', 'pattern', 'insight', 'correlation',
            'donnÃ©es', 'mÃ©trique', 'kpi', 'performance', 'Ã©volution',
            'comparaison', 'benchmark', 'croissance', 'diminution',
            'distribution', 'variance', 'Ã©cart-type'
        ]
    
    def can_handle(self, query: str, context: Dict[str, Any]) -> bool:
        """DÃ©termine si la requÃªte nÃ©cessite une analyse"""
        query_lower = query.lower()
        
        # VÃ©rifier les mots-clÃ©s analytiques
        has_analytical_keywords = any(keyword in query_lower for keyword in self.analytical_keywords)
        
        # VÃ©rifier la prÃ©sence de donnÃ©es tabulaires
        chunks = context.get('relevant_chunks', [])
        has_tables = any(chunk.get('chunk_type') == 'table' for chunk in chunks)
        
        return has_analytical_keywords or has_tables
    
    async def process(self, query: str, context: Dict[str, Any]) -> EngineResponse:
        """Traite une requÃªte analytique"""
        import time
        start_time = time.time()
        
        try:
            # Extraire et structurer les donnÃ©es
            structured_data = self._structure_data(context)
            
            prompt = self.analytical_prompt.format(
                query=query,
                context=structured_data
            )
            
            response = await self.llm.generate_response(prompt)
            
            processing_time = time.time() - start_time
            
            return EngineResponse(
                content=response,
                confidence=0.8,
                engine_type=self.engine_type,
                processing_time=processing_time,
                metadata={
                    "data_sources": len(context.get('relevant_chunks', [])),
                    "tables_found": len([c for c in context.get('relevant_chunks', []) if c.get('chunk_type') == 'table'])
                }
            )
            
        except Exception as e:
            logger.error(f"Erreur moteur analytique: {e}")
            return EngineResponse(
                content=f"Erreur lors de l'analyse: {str(e)}",
                confidence=0.0,
                engine_type=self.engine_type,
                processing_time=time.time() - start_time,
                metadata={"error": str(e)}
            )
    
    def _structure_data(self, context: Dict[str, Any]) -> str:
        """Structure les donnÃ©es pour l'analyse"""
        chunks = context.get('relevant_chunks', [])
        structured_parts = []
        
        # SÃ©parer par type de contenu
        tables = [c for c in chunks if c.get('chunk_type') == 'table']
        texts = [c for c in chunks if c.get('chunk_type') == 'text']
        
        if tables:
            structured_parts.append("=== DONNÃ‰ES TABULAIRES ===")
            for i, table in enumerate(tables, 1):
                structured_parts.append(f"Tableau {i}: {table.get('content', '')[:500]}...")
        
        if texts:
            structured_parts.append("=== INFORMATIONS CONTEXTUELLES ===")
            for text in texts[:2]:  # Limiter Ã  2 textes
                structured_parts.append(f"Contexte: {text.get('content', '')[:300]}...")
        
        return "\n".join(structured_parts)
    
    def get_capabilities(self) -> List[str]:
        return [
            "Analyse de tendances",
            "Identification de patterns",
            "Analyse comparative",
            "Insights mÃ©tiers",
            "Recommandations data-driven"
        ]

# ==================== SystÃ¨me Multi-Agents (SMA) ====================

class AgentCoordinator:
    """Coordinateur pour le systÃ¨me multi-agents"""
    
    def __init__(self, gemini_llm):
        self.llm = gemini_llm
        self.engines = {}
        self.decision_history = []
        
        # Prompt pour le routage intelligent
        self.routing_prompt = """
        Analysez cette requÃªte et dÃ©terminez quel(s) moteur(s) spÃ©cialisÃ©(s) 
        devrait(ent) la traiter.
        
        RequÃªte: {query}
        Moteurs disponibles: {available_engines}
        
        Pour chaque moteur, Ã©valuez:
        1. Pertinence (0-10)
        2. Confiance dans le traitement (0-10)
        3. Justification
        
        Format de rÃ©ponse:
        MOTEUR: [nom]
        PERTINENCE: [score]
        CONFIANCE: [score]
        JUSTIFICATION: [explication]
        
        RECOMMANDATION: [moteur principal recommandÃ©]
        """
    
    def register_engine(self, name: str, engine: SpecializedEngine):
        """Enregistre un moteur spÃ©cialisÃ©"""
        self.engines[name] = engine
        logger.info(f"Moteur {name} enregistrÃ© avec capacitÃ©s: {engine.get_capabilities()}")
    
    async def route_query(self, query: str, context: Dict[str, Any]) -> List[str]:
        """DÃ©termine quels moteurs utiliser pour une requÃªte"""
        available_engines = []
        
        # VÃ©rifier quels moteurs peuvent traiter la requÃªte
        for name, engine in self.engines.items():
            if engine.can_handle(query, context):
                available_engines.append({
                    "name": name,
                    "capabilities": engine.get_capabilities()
                })
        
        if not available_engines:
            return ["default"]  # Moteur par dÃ©faut
        
        # Si un seul moteur peut traiter, l'utiliser
        if len(available_engines) == 1:
            return [available_engines[0]["name"]]
        
        # Utiliser l'IA pour le routage intelligent
        try:
            engines_info = "\n".join([
                f"- {eng['name']}: {', '.join(eng['capabilities'])}"
                for eng in available_engines
            ])
            
            prompt = self.routing_prompt.format(
                query=query,
                available_engines=engines_info
            )
            
            response = await self.llm.generate_response(prompt)
            
            # Parser la rÃ©ponse pour extraire la recommandation
            recommended_engine = self._parse_routing_response(response, available_engines)
            
            return [recommended_engine] if recommended_engine else [available_engines[0]["name"]]
            
        except Exception as e:
            logger.error(f"Erreur routage: {e}")
            return [available_engines[0]["name"]]  # Fallback sur le premier disponible
    
    def _parse_routing_response(self, response: str, available_engines: List[Dict]) -> Optional[str]:
        """Parse la rÃ©ponse de routage de l'IA"""
        lines = response.strip().split('\n')
        
        for line in lines:
            if line.startswith("RECOMMANDATION:"):
                recommended = line.split(":", 1)[1].strip()
                # VÃ©rifier que le moteur recommandÃ© est disponible
                engine_names = [eng["name"] for eng in available_engines]
                for name in engine_names:
                    if name.lower() in recommended.lower():
                        return name
        
        return None
    
    async def process_with_engines(self, query: str, context: Dict[str, Any], 
                                 engine_names: List[str]) -> List[EngineResponse]:
        """Traite une requÃªte avec les moteurs spÃ©cifiÃ©s"""
        responses = []
        
        for engine_name in engine_names:
            if engine_name in self.engines:
                try:
                    response = await self.engines[engine_name].process(query, context)
                    responses.append(response)
                except Exception as e:
                    logger.error(f"Erreur moteur {engine_name}: {e}")
            elif engine_name == "default":
                # Traitement par dÃ©faut (moteur principal)
                responses.append(EngineResponse(
                    content="Traitement par le moteur principal",
                    confidence=0.7,
                    engine_type=EngineType.CONVERSATIONAL,
                    processing_time=0.0,
                    metadata={"engine": "default"}
                ))
        
        return responses

class MultiAgentResponseSynthesizer:
    """SynthÃ©tise les rÃ©ponses de plusieurs agents/moteurs"""
    
    def __init__(self, gemini_llm):
        self.llm = gemini_llm
        
        self.synthesis_prompt = """
        Vous devez synthÃ©tiser plusieurs rÃ©ponses d'experts spÃ©cialisÃ©s 
        pour crÃ©er une rÃ©ponse complÃ¨te et cohÃ©rente.
        
        Question originale: {query}
        
        RÃ©ponses des experts:
        {expert_responses}
        
        Instructions:
        1. IntÃ©grez les meilleures informations de chaque expert
        2. RÃ©solvez les Ã©ventuelles contradictions
        3. Organisez la rÃ©ponse de maniÃ¨re logique
        4. Indiquez les domaines d'expertise utilisÃ©s
        5. Mentionnez les niveaux de confiance
        
        RÃ©ponse synthÃ©tisÃ©e:
        """
    
    async def synthesize_responses(self, query: str, 
                                 responses: List[EngineResponse]) -> EngineResponse:
        """SynthÃ©tise plusieurs rÃ©ponses d'experts"""
        if not responses:
            return EngineResponse(
                content="Aucune rÃ©ponse disponible",
                confidence=0.0,
                engine_type=EngineType.CONVERSATIONAL,
                processing_time=0.0,
                metadata={}
            )
        
        if len(responses) == 1:
            return responses[0]
        
        try:
            # PrÃ©parer les rÃ©ponses des experts
            expert_responses = []
            total_confidence = 0
            
            for i, response in enumerate(responses, 1):
                expert_info = f"""
                Expert {i} ({response.engine_type.value}):
                Confiance: {response.confidence:.2f}
                RÃ©ponse: {response.content}
                ---
                """
                expert_responses.append(expert_info)
                total_confidence += response.confidence
            
            expert_text = "\n".join(expert_responses)
            
            prompt = self.synthesis_prompt.format(
                query=query,
                expert_responses=expert_text
            )
            
            synthesized = await self.llm.generate_response(prompt)
            
            # Calculer la confiance moyenne pondÃ©rÃ©e
            avg_confidence = total_confidence / len(responses)
            
            return EngineResponse(
                content=synthesized,
                confidence=min(avg_confidence, 1.0),
                engine_type=EngineType.CONVERSATIONAL,
                processing_time=sum(r.processing_time for r in responses),
                metadata={
                    "engines_used": [r.engine_type.value for r in responses],
                    "expert_count": len(responses),
                    "synthesis_method": "multi_expert"
                }
            )
            
        except Exception as e:
            logger.error(f"Erreur synthÃ¨se: {e}")
            # Fallback: retourner la rÃ©ponse avec la plus haute confiance
            best_response = max(responses, key=lambda r: r.confidence)
            return best_response

# ==================== IntÃ©gration avec l'Agent Principal ====================

class EnhancedRAGAgent:
    """Agent RAG amÃ©liorÃ© avec moteurs spÃ©cialisÃ©s et SMA"""
    
    def __init__(self, base_agent, gemini_llm):
        self.base_agent = base_agent
        self.coordinator = AgentCoordinator(gemini_llm)
        self.synthesizer = MultiAgentResponseSynthesizer(gemini_llm)
        
        # Enregistrer les moteurs spÃ©cialisÃ©s
        self._register_specialized_engines(gemini_llm)
    
    def _register_specialized_engines(self, gemini_llm):
        """Enregistre tous les moteurs spÃ©cialisÃ©s"""
        self.coordinator.register_engine("mathematical", MathematicalEngine(gemini_llm))
        self.coordinator.register_engine("technical", TechnicalEngine(gemini_llm))
        self.coordinator.register_engine("analytical", AnalyticalEngine(gemini_llm))
        
        logger.info("Moteurs spÃ©cialisÃ©s enregistrÃ©s")
    
    async def process_query_enhanced(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """Traite une requÃªte avec les moteurs spÃ©cialisÃ©s"""
        # Ã‰tape 1: Traitement de base avec l'agent principal
        base_result = await self.base_agent.process_query(query, session_id)
        
        # Ã‰tape 2: Routage vers les moteurs spÃ©cialisÃ©s
        context = {
            'relevant_chunks': base_result.get('sources', []),
            'search_intent': base_result.get('search_intent', ''),
            'expanded_queries': base_result.get('expanded_queries', [])
        }
        
        engine_names = await self.coordinator.route_query(query, context)
        
        # Ã‰tape 3: Traitement par les moteurs spÃ©cialisÃ©s
        specialized_responses = await self.coordinator.process_with_engines(
            query, context, engine_names
        )
        
        # Ã‰tape 4: SynthÃ¨se des rÃ©ponses
        if specialized_responses:
            # Ajouter la rÃ©ponse de base comme une rÃ©ponse d'expert
            base_response = EngineResponse(
                content=base_result['response'],
                confidence=base_result['confidence'],
                engine_type=EngineType.CONVERSATIONAL,
                processing_time=0.0,
                metadata={"engine": "base_rag"}
            )
            
            all_responses = [base_response] + specialized_responses
            final_response = await self.synthesizer.synthesize_responses(query, all_responses)
            
            # Mettre Ã  jour le rÃ©sultat
            base_result.update({
                'response': final_response.content,
                'confidence': final_response.confidence,
                'engines_used': final_response.metadata.get('engines_used', []),
                'specialized_processing': True,
                'synthesis_metadata': final_response.metadata
            })
        
        return base_result

# ==================== Factory et Configuration ====================

def create_enhanced_rag_agent(base_agent, gemini_api_key: str) -> EnhancedRAGAgent:
    """CrÃ©e un agent RAG amÃ©liorÃ© avec moteurs spÃ©cialisÃ©s"""
    from rag_agent_langgraph import GeminiLLM, GeminiConfig
    
    gemini_config = GeminiConfig(api_key=gemini_api_key)
    gemini_llm = GeminiLLM(gemini_config)
    
    return EnhancedRAGAgent(base_agent, gemini_llm)

# ==================== Exemple d'utilisation ====================

async def demo_specialized_engines():
    """DÃ©monstration des moteurs spÃ©cialisÃ©s"""
    from rag_agent_langgraph import create_rag_agent
    from pipeline import create_pipeline
    from config import RAGPipelineConfig
    
    # Configuration de base
    rag_config = RAGPipelineConfig()
    rag_pipeline = create_pipeline(rag_config)
    
    GEMINI_API_KEY = "your-gemini-api-key-here"
    
    # CrÃ©er l'agent de base
    base_agent = create_rag_agent(rag_pipeline, GEMINI_API_KEY)
    
    # CrÃ©er l'agent amÃ©liorÃ©
    enhanced_agent = create_enhanced_rag_agent(base_agent, GEMINI_API_KEY)
    
    # Tester diffÃ©rents types de requÃªtes
    test_queries = [
        "Calculez la moyenne des ventes du premier trimestre",  # MathÃ©matique + Analytique
        "Comment optimiser les performances d'une base de donnÃ©es?",  # Technique
        "Analysez les tendances de croissance dans les donnÃ©es",  # Analytique
        "Expliquez l'algorithme de tri rapide",  # Technique
        "Quelle est la probabilitÃ© de succÃ¨s du projet?"  # MathÃ©matique
    ]
    
    for query in test_queries:
        print(f"\nğŸ” RequÃªte: {query}")
        print("=" * 60)
        
        result = await enhanced_agent.process_query_enhanced(query)
        
        print(f"ğŸ¤– Moteurs utilisÃ©s: {result.get('engines_used', ['base'])}")
        print(f"ğŸ“Š Confiance: {result['confidence']:.3f}")
        print(f"ğŸ¯ Traitement spÃ©cialisÃ©: {result.get('specialized_processing', False)}")
        print(f"ğŸ“ RÃ©ponse: {result['response'][:300]}...")
        
        await asyncio.sleep(1)

if __name__ == "__main__":
    asyncio.run(demo_specialized_engines())